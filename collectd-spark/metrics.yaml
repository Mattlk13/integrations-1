counter.HiveExternalCatalog.counter.HiveClientCalls:
  brief: Total number of client calls sent to Hive for query processing
  description: The total number of client calls sent to Hive for query processing.
    This metric is reported with the dimension `spark_process` to indicate whether
    it corresponds to a master or worker process.
  metric_type: counter
  title: Hive Client Calls

counter.HiveExternalCatalog.fileCacheHits:
  brief: Total number of file level cache hits occurred
  description: The total number of file level cache hits occurred. This metric is
    reported with the dimension `spark_process` to indicate whether it corresponds
    to a master or worker process.
  metric_type: counter
  title: File Cache Hits

counter.HiveExternalCatalog.filesDiscovered:
  brief: Total number of files discovered
  description: The total number of files discovered. This metric is reported with
    the dimension `spark_process` to indicate whether it corresponds to a master or
    worker process.
  metric_type: counter
  title: Files Discovered

counter.HiveExternalCatalog.parallelListingJobCount:
  brief: Total number of Hive-specific jobs running in parallel
  description: The total number of Hive-specific jobs running in parallel. This metric
    is reported with the dimension `spark_process` to indicate whether it corresponds
    to a master or worker process.
  metric_type: counter
  title: Parallel Listing Job Count

counter.HiveExternalCatalog.partitionsFetched:
  brief: Total number of partitions fetched
  description: The total number of partitions fetched in Hive. This metric is reported
    with the dimension `spark_process` to indicate whether it corresponds to a master
    or worker process.
  metric_type: counter
  title: Number of Partitions Fetched

counter.spark.driver.completed_tasks:
  brief: Total number of completed tasks in driver mapped to a particular application
  description: Total number of completed tasks in driver mapped to a particular application.
    This metric is reported with the dimension `cluster` to specify the cluster Spark
    is running on (e.g. Mesos)
  metric_type: counter
  title: Driver Completed Tasks

counter.spark.driver.disk_used:
  brief: Amount of disk used by driver mapped to a particular application
  description: Amount of disk used by driver mapped to a particular application (expressed
    in bytes). This metric is reported with the dimension `cluster` to specify the
    cluster Spark is running on (e.g. Mesos)
  metric_type: counter
  title: Driver Disk Used

counter.spark.driver.failed_tasks:
  brief: Total number of failed tasks in driver mapped to a particular application
  description: Total number of failed tasks in driver mapped to a particular application.
    This metric is reported with the dimension `cluster` to specify the cluster Spark
    is running on (e.g. Mesos)
  metric_type: counter
  title: Driver Failed Tasks

counter.spark.driver.memory_used:
  brief: Amount of memory used by driver mapped to a particular application
  description: Amount of memory used by driver mapped to a particular application
    (expressed in bytes). This metric is reported with the dimension `cluster` to
    specify the cluster Spark is running on (e.g. Mesos)
  metric_type: counter
  title: Driver Memory Used

counter.spark.driver.total_duration:
  brief: Fraction of time spent by driver mapped to a particular application
  description: Fraction of time spent by driver mapped to a particular application
    (expressed in ms/s). This metric is reported with the dimension `cluster` to specify
    the cluster Spark is running on (e.g. Mesos)
  metric_type: counter
  title: Driver Total Duration

counter.spark.driver.total_input_bytes:
  brief: Number of input bytes in driver mapped to a particular application
  description: The number of input bytes in driver mapped to a particular application
    (expressed in bytes). This metric is reported with the dimension `cluster` to
    specify the cluster Spark is running on (e.g. Mesos)
  metric_type: counter
  title: Driver Total Input Bytes

counter.spark.driver.total_shuffle_read:
  brief: Size read during a shuffle in driver mapped to a particular application
  description: The size read during a shuffle in driver mapped to a particular application(expressed
    in bytes). This metric is reported with the dimension `cluster` to specify the
    cluster Spark is running on (e.g. Mesos)
  metric_type: counter
  title: Driver Total Shuffle Read

counter.spark.driver.total_shuffle_write:
  brief: Size written to during a shuffle in driver mapped to a particular application
  description: The size written to during a shuffle in driver mapped to a particular
    application (expressed in bytes). This metric is reported with the dimension `cluster`
    to specify the cluster Spark is running on (e.g. Mesos)
  metric_type: counter
  title: Driver Total Shuffle Write

counter.spark.driver.total_tasks:
  brief: Total number of tasks in driver mapped to a particular application
  description: Total number of tasks in driver mapped to a particular application.
    This metric is reported with the dimension `cluster` to specify the cluster Spark
    is running on (e.g. Mesos)
  metric_type: counter
  title: Driver Total Tasks

counter.spark.executor.completed_tasks:
  brief: Completed tasks across executors working for a particular application
  description: The number of completed tasks across executors working for a particular
    application. This metric is reported with the dimension `cluster` to specify the
    cluster Spark is running on (e.g. Mesos)
  metric_type: counter
  title: Completed Tasks Aggregated across Executors

counter.spark.executor.disk_used:
  brief: Amount of disk used across executors working for a particular application
  description: Amount of disk used across executors working for a particular application
    (expressed in bytes). This metric is reported with the dimension `cluster` to
    specify the cluster Spark is running on (e.g. Mesos)
  metric_type: counter
  title: Disk Used Aggregated across Executors

counter.spark.executor.failed_tasks:
  brief: Failed tasks across executors working for a particular application
  description: The number of failed tasks across executors working for a particular
    application. This metric is reported with the dimension `cluster` to specify the
    cluster Spark is running on (e.g. Mesos)
  metric_type: counter
  title: Failed Tasks Aggregated across Executors

counter.spark.executor.memory_used:
  brief: Amount of memory used across executors working for a particular application
  description: Amount of memory used across executors working for a particular application
    (expressed in bytes). This metric is reported with the dimension `cluster` to
    specify the cluster Spark is running on (e.g. Mesos)
  metric_type: counter
  title: Memory Used Aggregated across Executors

counter.spark.executor.total_duration:
  brief: Fraction of time spent across executors working for a particular application
  description: Fraction of time spent across executors working for a particular application
    (expressed in ms/s). This metric is reported with the dimension `cluster` to specify
    the cluster Spark is running on (e.g. Mesos)
  metric_type: counter
  title: Total Duration Aggregated across Executors

counter.spark.executor.total_input_bytes:
  brief: Number of input bytes across executors working for a particular application
  description: The number of input bytes across executors working for a particular
    application (expressed in bytes). This metric is reported with the dimension `cluster`
    to specify the cluster Spark is running on (e.g. Mesos)
  metric_type: counter
  title: Input Bytes Aggregated across Executors

counter.spark.executor.total_shuffle_read:
  brief: Size read during a shuffle in a particular application's executors
  description: The size read during a shuffle in a particular application's executors
    (expressed in bytes) - aggregated across executors. This metric is reported with
    the dimension `cluster` to specify the cluster Spark is running on (e.g. Mesos)
  metric_type: counter
  title: Shuffle Read Aggregated across Executors

counter.spark.executor.total_shuffle_write:
  brief: Size written to during a shuffle in a particular application's executors
  description: The size written to during a shuffle in a particular application's
    executors (expressed in bytes) - aggregated across executors. This metric is reported
    with the dimension `cluster` to specify the cluster Spark is running on (e.g.
    Mesos)
  metric_type: counter
  title: Shuffle Write Aggregated across Executors

counter.spark.executor.total_tasks:
  brief: Total tasks across executors working for a particular application
  description: The total number of tasks across executors working for a particular
    application (expressed in bytes). This metric is reported with the dimension `cluster`
    to specify the cluster Spark is running on (e.g. Mesos)
  metric_type: counter
  title: Total Tasks Aggregated across Executors

counter.spark.streaming.num_processed_records:
  brief: Number of processed records in a streaming application
  description: The number of processed records in a streaming application. This metric
    is reported with the dimension `cluster` to specify the cluster Spark is running
    on (e.g. Mesos)
  metric_type: counter
  title: Number of Processed Records

counter.spark.streaming.num_received_records:
  brief: Number of received records in a streaming application
  description: The number of received records in a streaming application. This metric
    is reported with the dimension `cluster` to specify the cluster Spark is running
    on (e.g. Mesos)
  metric_type: counter
  title: Number of Received Records

counter.spark.streaming.num_total_completed_batches:
  brief: Number of batches completed in a streaming application
  description: The number of batches completed in a streaming application. This metric
    is reported with the dimension `cluster` to specify the cluster Spark is running
    on (e.g. Mesos)
  metric_type: counter
  title: Total Completed Batches

gauge.jvm.MarkSweepCompact.count:
  brief: Garbage collection count
  description: The number of times garbage collection have occurred in the Marksweep
    GC. This metric is reported with the dimension `spark_process` to indicate whether
    it corresponds to a master or worker process.
  metric_type: gauge
  title: Mark Sweep Compact Count

gauge.jvm.MarkSweepCompact.time:
  brief: Garbage collection time
  description: The time taken for garbage collection that have occurred in the Marksweep
    GC. This metric is reported with the dimension `spark_process` to indicate whether
    it corresponds to a master or worker process.
  metric_type: gauge
  title: Mark Sweep Compact Time

gauge.jvm.heap.committed:
  brief: Amount of committed heap memory (in MB)
  description: The total amount of heap memory (expressed in MB) committed. This metric
    is reported with the dimension `spark_process` to indicate whether it corresponds
    to a master or worker process.
  metric_type: gauge
  title: JVM Heap Memory Committed

gauge.jvm.heap.used:
  brief: Amount of used heap memory (in MB)
  description: The total amount of heap memory (expressed in MB) used. This metric
    is reported with the dimension `spark_process` to indicate whether it corresponds
    to a master or worker process.
  metric_type: gauge
  title: JVM Heap Memory Used

gauge.jvm.non-heap.committed:
  brief: Amount of committed non-heap memory (in MB)
  description: The total amount of non-heap memory (expressed in MB) committed. This
    metric is reported with the dimension `spark_process` to indicate whether it corresponds
    to a master or worker process.
  metric_type: gauge
  title: JVM Non-Heap Memory Committed

gauge.jvm.non-heap.used:
  brief: Amount of used non-heap memory (in MB)
  description: The total amount of non-heap memory (expressed in MB) used. This metric
    is reported with the dimension `spark_process` to indicate whether it corresponds
    to a master or worker process.
  metric_type: gauge
  title: JVM Non-Heap Memory Used

gauge.jvm.pools.Code-Cache.committed:
  brief: Amount of memory committed for compilation and storage of native code
  description: The amount of memory (expressed in MB) committed for compilation and
    storage of native code. This metric is reported with the dimension `spark_process`
    to indicate whether it corresponds to a master or worker process.
  metric_type: gauge
  title: Code-Cache Memory Committed

gauge.jvm.pools.Code-Cache.used:
  brief: Amount of memory used to compile and store native code
  description: The amount of memory (expressed in MB) used to compile and store native
    code. This metric is reported with the dimension `spark_process` to indicate whether
    it corresponds to a master or worker process.
  metric_type: gauge
  title: Code-Cache Memory Used

gauge.jvm.pools.Compressed-Class-Space.committed:
  brief: Amount of memory committed for compressing a class object
  description: The amount of memory (expressed in MB) committed for compressing a
    class object. This metric is reported with the dimension `spark_process` to indicate
    whether it corresponds to a master or worker process.
  metric_type: gauge
  title: Compressed-Class-Space Memory Committed

gauge.jvm.pools.Compressed-Class-Space.used:
  brief: Amount of memory used to compress a class object
  description: The amount of memory (expressed in MB) used to compress a class object.
    This metric is reported with the dimension `spark_process` to indicate whether
    it corresponds to a master or worker process.
  metric_type: gauge
  title: Compressed-Class-Space Memory Used

gauge.jvm.pools.Eden-Space.committed:
  brief: Amount of memory committed for the initial allocation of objects
  description: The amount of memory (expressed in MB) committed for the initial allocation
    of objects. This metric is reported with the dimension `spark_process` to indicate
    whether it corresponds to a master or worker process.
  metric_type: gauge
  title: Eden-Space Memory Committed

gauge.jvm.pools.Eden-Space.used:
  brief: Amount of memory used for the initial allocation of objects
  description: The amount of memory (expressed in MB) used for the initial allocation
    of objects. This metric is reported with the dimension `spark_process` to indicate
    whether it corresponds to a master or worker process.
  metric_type: gauge
  title: Eden-Space Memory Used

gauge.jvm.pools.Metaspace.committed:
  brief: Amount of memory committed for storing classes and classloaders
  description: The amount of memory (expressed in MB) committed for storing classes
    and classloaders. This metric is reported with the dimension `spark_process` to
    indicate whether it corresponds to a master or worker process.
  metric_type: gauge
  title: Metaspace Memory Committed

gauge.jvm.pools.Metaspace.used:
  brief: Amount of memory used to store classes and classloaders
  description: The amount of memory (expressed in MB) used to store classes and classloaders.
    This metric is reported with the dimension `spark_process` to indicate whether
    it corresponds to a master or worker process.
  metric_type: gauge
  title: Metaspace Memory Used

gauge.jvm.pools.Survivor-Space.committed:
  brief: Amount of memory committed specifically for objects that have survived GC
    of the Eden Space
  description: Amount of memory (expressed in MB) committed specifically for objects
    that have survived garbace collection of the Eden Space. This metric is reported
    with the dimension `spark_process` to indicate whether it corresponds to a master
    or worker process.
  metric_type: gauge
  title: Survivor-Space Memory Committed

gauge.jvm.pools.Survivor-Space.used:
  brief: Amount of memory used for objects that have survived GC of the Eden Space
  description: Amount of memory (expressed in MB) used for objects that have survived
    garbace collection of the Eden Space. This metric is reported with the dimension
    `spark_process` to indicate whether it corresponds to a master or worker process.
  metric_type: gauge
  title: Survivor-Space Memory Used

gauge.jvm.pools.Tenured-Gen.committed:
  brief: Amount of memory committed to store objects that have lived in the survivor
    space for a given period of time
  description: Amount of memory (expressed in MB) committed to store objects that
    have lived in the survivor space for a given period of time. This metric is reported
    with the dimension `spark_process` to indicate whether it corresponds to a master
    or worker process.
  metric_type: gauge
  title: Tenured-Gen Memory Committed

gauge.jvm.pools.Tenured-Gen.used:
  brief: Amount of memory used for objects that have lived in the survivor space for
    a given period of time
  description: Amount of memory (expressed in MB) used for objects that have lived
    in the survivor space for a given period of time. This metric is reported with
    the dimension `spark_process` to indicate whether it corresponds to a master or
    worker process.
  metric_type: gauge
  title: Tenured-Gen Memory Used

gauge.jvm.total.committed:
  brief: Amount of committed JVM memory (in MB)
  description: The total amount of JVM memory (expressed in MB) committed. This metric
    is reported with the dimension `spark_process` to indicate whether it corresponds
    to a master or worker process.
  metric_type: gauge
  title: JVM Memory Committed

gauge.jvm.total.used:
  brief: Amount of used JVM memory (in MB)
  description: The total amount of JVM memory (expressed in MB) used. This metric
    is reported with the dimension `spark_process` to indicate whether it corresponds
    to a master or worker process.
  metric_type: gauge
  title: JVM Memory Used

gauge.master.aliveWorkers:
  brief: Total functioning workers
  description: The total number of functioning workers reporting to the master process.
    This metric is reported with the dimension `spark_process` to indicate whether
    it corresponds to a master or worker process.
  metric_type: gauge
  title: Number of Alive Workers

gauge.master.apps:
  brief: Total number of active applications in the spark cluster
  description: The total number of applications still active/running in the spark
    cluster. This metric is reported with the dimension `spark_process` to indicate
    whether it corresponds to a master or worker process.
  metric_type: gauge
  title: Number of Active Applications

gauge.master.waitingApps:
  brief: Total number of waiting applications in the spark cluster
  description: The total number of applications in queue waiting to be executed in
    the spark cluster. This metric is reported with the dimension `spark_process`
    to indicate whether it corresponds to a master or worker process.
  metric_type: gauge
  title: Number of Waiting Applications

gauge.master.workers:
  brief: Total number of workers in spark cluster
  description: The total number of workers in spark cluster registered to the master
    process. This metric is reported with the dimension `spark_process` to indicate
    whether it corresponds to a master or worker process.
  metric_type: gauge
  title: Total Number of Workers

gauge.spark.driver.active_tasks:
  brief: Total number of active tasks in driver mapped to a particular application
  description: Total number of active tasks in driver mapped to a particular application.
    This metric is reported with the dimension `cluster` to specify the cluster Spark
    is running on (e.g. Mesos)
  metric_type: gauge
  title: Driver Active Tasks

gauge.spark.driver.max_memory:
  brief: Maximum memory used by driver mapped to a particular application
  description: Maximum memory used by driver mapped to a particular application (express
    in bytes). This metric is reported with the dimension `cluster` to specify the
    cluster Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Driver Max Memory

gauge.spark.driver.rdd_blocks:
  brief: Number of RDD blocks in the driver mapped to a particular application
  description: Number of RDD blocks in the driver mapped to a particular application.
    This metric is reported with the dimension `cluster` to specify the cluster Spark
    is running on (e.g. Mesos)
  metric_type: gauge
  title: Driver RDD Blocks

gauge.spark.executor.active_tasks:
  brief: Total number of active tasks across all executors working for a particular
    application
  description: Total number of active tasks across all executors working for a particular
    application. This metric is reported with the dimension `cluster` to specify the
    cluster Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Active Tasks Aggregated across Executors

gauge.spark.executor.count:
  brief: Total number of executors performing for an active application in the spark
    cluster
  description: Total number of executors performing for an active application in the
    spark cluster. This metric is reported with the dimension `cluster` to specify
    the cluster Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Total Number of Executors

gauge.spark.executor.max_memory:
  brief: Max memory across all executors working for a particular application
  description: Max memory across all executors working for a particular application
    (expressed in bytes). This metric is reported with the dimension `cluster` to
    specify the cluster Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Max Memory Aggregated across Executors

gauge.spark.executor.rdd_blocks:
  brief: Number of RDD blocks across all executors working for a particular application
  description: Number of RDD blocks across all executors working for a particular
    application. This metric is reported with the dimension `cluster` to specify the
    cluster Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: RDD Blocks Aggregated across Executors

gauge.spark.job.num_active_stages:
  brief: Total number of active stages for an active application in the spark cluster
  description: The total number of active stages for an active application in the
    spark cluster - aggregated by jobs. This metric is reported with the dimension
    `cluster` to specify the cluster Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Total Number of Active Stages

gauge.spark.job.num_active_tasks:
  brief: Total number of active tasks for an active application in the spark cluster
  description: The total number of active tasks for an active application in the spark
    cluster - aggregated by jobs. This metric is reported with the dimension `cluster`
    to specify the cluster Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Total Number of Active Tasks

gauge.spark.job.num_completed_stages:
  brief: Total number of completed stages for an active application in the spark cluster
  description: The total number of completed stages for an active application in the
    spark cluster - aggregated by jobs. This metric is reported with the dimension
    `cluster` to specify the cluster Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Total Number of Completed Stages

gauge.spark.job.num_completed_tasks:
  brief: Total number of completed tasks for an active application in the spark cluster
  description: The total number of completed tasks for an active application in the
    spark cluster - aggregated by jobs. This metric is reported with the dimension
    `cluster` to specify the cluster Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Total Number of Completed Tasks

gauge.spark.job.num_failed_stages:
  brief: Total number of failed stages for an active application in the spark cluster
  description: The total number of failed stages for an active application in the
    spark cluster - aggregated by jobs. This metric is reported with the dimension
    `cluster` to specify the cluster Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Total Number of Failed Stages

gauge.spark.job.num_failed_tasks:
  brief: Total number of failed tasks for an active application in the spark cluster
  description: The total number of failed tasks for an active application in the spark
    cluster - aggregated by jobs. This metric is reported with the dimension `cluster`
    to specify the cluster Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Total Number of Failed Tasks

gauge.spark.job.num_skipped_stages:
  brief: Total number of skipped stages for an active application in the spark cluster
  description: The total number of skipped stages for an active application in the
    spark cluster - aggregated by jobs. This metric is reported with the dimension
    `cluster` to specify the cluster Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Total Number of Skipped Stages

gauge.spark.job.num_skipped_tasks:
  brief: Total number of skipped tasks for an active application in the spark cluster
  description: The total number of skipped tasks for an active application in the
    spark cluster - aggregated by jobs. This metric is reported with the dimension
    `cluster` to specify the cluster Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Total Number of Skipped Tasks

gauge.spark.job.num_tasks:
  brief: Total number of tasks for an active application in the spark cluster
  description: The total number of tasks for an active application in the spark cluster
    - aggregated by jobs. This metric is reported with the dimension `cluster` to
    specify the cluster Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Total Number of Tasks

gauge.spark.num_active_stages:
  brief: Total number of active stages for an active application in the spark cluster
  description: The total number of active stages for an active application in the
    spark cluster. This metric is reported with the dimension `cluster` to specify
    the cluster Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Total Number of Active Stages

gauge.spark.num_running_jobs:
  brief: Total number of running jobs for an active application in the spark cluster
  description: The total number of running jobs for an active application in the spark
    cluster. This metric is reported with the dimension `cluster` to specify the cluster
    Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Total Number of Running Jobs

gauge.spark.stage.disk_bytes_spilled:
  brief: Actual size written to disk for an active application in the spark cluster
  description: Actual size written to disk (expressed in bytes) - aggregated by active
    stages. This metric is reported with the dimension `cluster` to specify the cluster
    Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Disk Bytes Spilled

gauge.spark.stage.executor_run_time:
  brief: Fraction of time spent by (and averaged across) executors for a particular
    application
  description: Fraction of time spent by (and averaged across) executors for a particular
    application (expressed in ms/s) - aggregated by active stages. This metric is
    reported with the dimension `cluster` to specify the cluster Spark is running
    on (e.g. Mesos)
  metric_type: gauge
  title: Average Executor Run Time

gauge.spark.stage.input_bytes:
  brief: Input size for a particular application
  description: Input size for a particular application (expressed as bytes) - aggregated
    by active stages. This metric is reported with the dimension `cluster` to specify
    the cluster Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Stage Input Bytes

gauge.spark.stage.input_records:
  brief: Input records received for a particular application
  description: Input records received for a particular application - aggregated by
    active stages. This metric is reported with the dimension `cluster` to specify
    the cluster Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Stage Input Records

gauge.spark.stage.memory_bytes_spilled:
  brief: Size spilled to disk from memory for an active application in the spark cluster
  description: Size spilled to disk from memory (expressed in bytes) - aggregated
    by active stages. This metric is reported with the dimension `cluster` to specify
    the cluster Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Memory Bytes Spilled

gauge.spark.stage.output_bytes:
  brief: Output size for a particular application
  description: Output size for a particular application (expressed as bytes) - aggregated
    by active stages. This metric is reported with the dimension `cluster` to specify
    the cluster Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Stage Output Bytes

gauge.spark.stage.output_records:
  brief: Output records written to for a particular application
  description: Output records written to for a particular application - aggregated
    by active stages. This metric is reported with the dimension `cluster` to specify
    the cluster Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Stage Output Records

gauge.spark.stage.shuffle_read_bytes:
  brief: Read size during shuffle phase for a particular application
  description: Read size during shuffle phase for a particular application (expressed
    as bytes) - aggregated by active stages. This metric is reported with the dimension
    `cluster` to specify the cluster Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Stage Shuffle Read Bytes

gauge.spark.stage.shuffle_read_records:
  brief: Number of records read during shuffle phase for a particular application
  description: Number of records read during shuffle phase for a particular application
    - aggregated by active stages. This metric is reported with the dimension `cluster`
    to specify the cluster Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Stage Shuffle Read Records

gauge.spark.stage.shuffle_write_bytes:
  brief: Size written during shuffle phase for a particular application
  description: Size written during shuffle phase for a particular application (expressed
    as bytes) - aggregated by active stages. This metric is reported with the dimension
    `cluster` to specify the cluster Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Stage Shuffle Write Bytes

gauge.spark.stage.shuffle_write_records:
  brief: Number of records written to during shuffle phase for a particular application
  description: Number of records written to during shuffle phase for a particular
    application - aggregated by active stages. This metric is reported with the dimension
    `cluster` to specify the cluster Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Stage Shuffle Write Records

gauge.spark.streaming.avg_input_rate:
  brief: Average input rate of records across retained batches in a streaming application
  description: The average input rate of records across retained batches in a streaming
    application (expressed in ms/s). This metric is reported with the dimension `cluster`
    to specify the cluster Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Average Input Rate

gauge.spark.streaming.avg_processing_time:
  brief: Average processing time in a streaming application
  description: The average processing time in a streaming application (expressed in
    ms). This metric is reported with the dimension `cluster` to specify the cluster
    Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Average Processing Time

gauge.spark.streaming.avg_scheduling_delay:
  brief: Average scheduling delay in a streaming application
  description: The average scheduling delay in a streaming application (expressed
    in ms). This metric is reported with the dimension `cluster` to specify the cluster
    Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Average Scheduling Delay

gauge.spark.streaming.avg_total_delay:
  brief: Average total delay in a streaming application
  description: The average total delay in a streaming application (expressed in ms).
    This metric is reported with the dimension `cluster` to specify the cluster Spark
    is running on (e.g. Mesos)
  metric_type: gauge
  title: Average Total Delay

gauge.spark.streaming.num_active_batches:
  brief: Number of active batches in a streaming application
  description: The number of active batches in a streaming application. This metric
    is reported with the dimension `cluster` to specify the cluster Spark is running
    on (e.g. Mesos)
  metric_type: gauge
  title: Number of Active Batches

gauge.spark.streaming.num_inactive_receivers:
  brief: Number of inactive receivers in a streaming application
  description: The number of receivers (e.g. Kafka, Flume, etc.) that have become
    inactive in a streaming application. This metric is reported with the dimension
    `cluster` to specify the cluster Spark is running on (e.g. Mesos)
  metric_type: gauge
  title: Number of Inactive Receivers

gauge.worker.coresFree:
  brief: Total cores free for a particular worker process
  description: The total number of cores free for a particular worker process. This
    metric is reported with the dimension `spark_process` to indicate whether it corresponds
    to a master or worker process.
  metric_type: gauge
  title: Number of Cores Free

gauge.worker.coresUsed:
  brief: Total cores used by a particular worker process
  description: The total number of cores used by a particular worker process. This
    metric is reported with the dimension `spark_process` to indicate whether it corresponds
    to a master or worker process.
  metric_type: gauge
  title: Number of Cores Used

gauge.worker.executors:
  brief: Total number of executors for a particular worker process
  description: The total number of executors running jobs for a particular worker
    process. This metric is reported with the dimension `spark_process` to indicate
    whether it corresponds to a master or worker process.
  metric_type: gauge
  title: Number of Executors

gauge.worker.memFree_MB:
  brief: Total memory free for a particular worker process
  description: The total amount of memory (expressed in MB) available to a particular
    worker process. This metric is reported with the dimension `spark_process` to
    indicate whether it corresponds to a master or worker process.
  metric_type: gauge
  title: Total Memory Free

gauge.worker.memUsed_MB:
  brief: Memory used by a particular worker process
  description: The amount of memory (expressed in MB) currently used by a particular
    worker process. This metric is reported with the dimension `spark_process` to
    indicate whether it corresponds to a master or worker process.
  metric_type: gauge
  title: Memory Used

